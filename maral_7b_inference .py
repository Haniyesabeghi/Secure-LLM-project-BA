# -*- coding: utf-8 -*-
"""Copy of Maral_7B_Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XKV7tJnDp3mMgilSmtS1-zRe_VDcj0D5

# Maral 7B Inference Notebook

<p align="center">
 <img src="https://huggingface.co/MaralGPT/Maral-7B-alpha-1/resolve/main/maral-7b-announce.png" width=256 height=256/>
</p>

## About Maral

Maral is just a new large lanugage model, specializing on the Persian language. This model is based on Mistral and trained an Alpaca Persian dataset. This model is one of the few efforts in Persian speaking scene in order to bring our language to a new life in the era of AI.

Also, since Maral is based on Mistral, it's capable of producing English answers as well.

## Our Team

* Muhammadreza Haghiri ([Website](https://haghiri75.com/en) - [Github](https://github.com/prp-e) - [LinkedIn](https://www.linkedin.com/in/muhammadreza-haghiri-1761325b))
* Mahi Mohrechi ([Website](https://mohrechi-portfolio.vercel.app/) - [Github](https://github.com/f-mohrechi) - [LinkedIn](https://www.linkedin.com/in/faeze-mohrechi/))

## Needed libraries

Since the model is loaded in 8 bit quantization mode on free colab, you need `bitsandbytes`. If you do own a better GPU, go with full 16 bit quantization.
"""

!pip install transformers accelerate bitsandbytes -q

!pip install pandas -q

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch
import pandas as pd
from google.colab import files
import io

#Ù…Ø±Ø­Ù„Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø±
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ (.txt) Ø®ÙˆØ¯ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")

# Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ txt
while True:
    uploaded = files.upload()

    if not uploaded:
        print("âš ï¸ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯! Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
        continue

    # Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ø¢Ù†
    file_name = list(uploaded.keys())[0]
    file_content = uploaded[file_name]

    # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø³ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„
    if not file_name.lower().endswith('.txt'):
        print(f"âŒ ÙØ§ÛŒÙ„ '{file_name}' Ø§Ø² Ù†ÙˆØ¹ txt Ù†ÛŒØ³Øª!")
        print("Ù„Ø·ÙØ§Ù‹ ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ .txt Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
        print("Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯...\n")
        continue

    break  # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ txt Ø¨ÙˆØ¯ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø®Ø§Ø±Ø¬ Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…

print(f"âœ… ÙØ§ÛŒÙ„ txt Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡: {file_name}")

# Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„ txt
try:
    # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ø¨Ù‡ Ù…ØªÙ†
    text_content = file_content.decode('utf-8')  # ÛŒØ§ 'utf-8-sig' Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ BOM
except UnicodeDecodeError:
    try:
        text_content = file_content.decode('utf-8-sig')
    except UnicodeDecodeError:
        text_content = file_content.decode('latin-1')  # ÛŒØ§ encodingÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±

# ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø®Ø·ÙˆØ· Ùˆ Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ
prompts = [line.strip() for line in text_content.split('\n') if line.strip()]

if not prompts:
    print("âš ï¸ Ù‡ÛŒÚ† Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
else:
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    dataset = pd.DataFrame({'prompt': prompts})

    # Ù†Ù…Ø§ÛŒØ´ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ
    print(f"\nğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±Ø§Ù…Ù¾Øªâ€ŒÙ‡Ø§: {len(dataset)}")
    print("\nğŸ“ Ûµ Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§Ø³Øª:")
    display(dataset.head())  # Ø¯Ø± Ú©ÙˆÙ„Ø¨ Ø¨Ù‡ØªØ± Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯

    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…ØªÙ†â€ŒÙ‡Ø§
    print("\nğŸ” Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:")
    for i, prompt in enumerate(prompts[:3]):
        print(f"{i+1}. {prompt[:100]}..." if len(prompt) > 100 else f"{i+1}. {prompt}")

"""## Loading Model"""

model_name_or_id = "MaralGPT/Maral-7B-alpha-1"

model = AutoModelForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, device_map="auto", low_cpu_mem_usage=True, load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_id)

"""## Model Structure"""

model

"""### Generation Config

This cell, is a simple and easy way to tweak the configurations for text generation.
"""

generation_config = GenerationConfig(
    do_sample=True,
    top_k=1,
    temperature=0.9,
    max_new_tokens=100,
    pad_token_id=tokenizer.eos_token_id
)

"""## Prompt Format

This model, uses _Guanaco_ format, which is like this:

```
### Human: <prompt>
### Assistant: <answer>
```

So in the below cell, you can easily modify the prompt without messing with the format.
"""

# Ø­Ø§Ù„Ø§ Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª
outputs = []  # Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§

for index, row in dataset.iterrows():
    prompt = row['prompt']  # Ù¾Ø±Ø§Ù…Ù¾Øª ÙØ¹Ù„ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª

    # Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ù¾Ø±Ø§Ù…Ù¾Øª Ø±Ùˆ ÙØ±Ù…Øª Ú©Ù†ÛŒ (Ù…Ø«Ù„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† "Human:" ÛŒØ§ Ú†ÛŒØ²ÛŒ)
    formatted_prompt = f"### Human: {prompt}\n### Assistant:"

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ inputs
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")  # Ø§Ú¯Ø± GPU Ø¯Ø§Ø±ÛŒØŒ "cuda" Ù†Ú¯Ù‡ Ø¯Ø§Ø±

    # Ø¬Ù†Ø±ÛŒØª
    output_ids = model.generate(**inputs, generation_config=generation_config)

    # decode Ø®Ø±ÙˆØ¬ÛŒ
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # ÙÙ‚Ø· Ø¨Ø®Ø´ Assistant Ø±Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒØŒ Ø¨Ø³ØªÙ‡ Ø¨Ù‡ ÙØ±Ù…Øª)
    if "### Assistant:" in generated_text:
        generated_text = generated_text.split("### Assistant:")[1].strip()

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª
    outputs.append({'prompt': prompt, 'response': generated_text})

    print(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø§Ù…Ù¾Øª {index+1}/{len(dataset)} ØªÙ…Ø§Ù… Ø´Ø¯.")

# ØªØ¨Ø¯ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
with open('Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„.txt', 'w', encoding='utf-8') as f:
    for i, item in enumerate(outputs, 1):
        f.write(f"Ø´Ù…Ø§Ø±Ù‡ {i}\n")
        f.write(f"Ù¾Ø±Ø§Ù…Ù¾Øª: {item['prompt']}\n")
        f.write(f"Ù¾Ø§Ø³Ø® : {item['response']}\n")
        f.write("\n" + "="*80 + "\n\n")

print("ÙØ§ÛŒÙ„ 'Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„.txt' Ø¢Ù…Ø§Ø¯Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø³Øª.")